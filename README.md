# Kafka Connector

A **Go framework for building Kafka connectors (source and sink)**.
It supports **configuration fully compatible with the Kafka Connect REST API**, and **connectors are deployed as Go plugins** for flexible extension. The framework also includes encoding/decoding support, validation, and Docker-ready deployment.

Use it to:

* Stream data **into Kafka** (source connectors).
* Stream data **out of Kafka** (sink connectors).
* Apply transformations and validations.
* Deploy connectors as **Go plugins**.
* Manage tasks with sink/source runners.

---

## ğŸš€ Features

* **Source & Sink connectors** â€” read from and write to Kafka topics.
* **Config compatibility** â€” fully aligned with Kafka Connect REST API configs.
* **Go plugin deployment** â€” connectors are compiled and loaded as Go plugins.
* **Encoding/decoding support** â€” flexible data formats.
* **Pluggable design** â€” extend via registry and plugins.
* **Validation** â€” built-in config validation.
* **Container-ready** â€” deploy easily with Docker.

---

## ğŸ“¦ Getting Started

### Prerequisites

* Go 1.23+
* Docker (optional, for containerized deployment)
* Running Kafka cluster

### Clone the repo

```bash
git clone https://github.com/noelyahan/kafka-connector.git
cd kafka-connector
```

### Build

```bash
go build ./...
```

### Run

```bash
go run main.go
```

---

## âš™ï¸ Configuration

Connector configuration is **fully compatible with the Kafka Connect REST API**.

Example (same format as youâ€™d POST to Kafka Connect):

```json
{
  "name": "sample-sink",
  "config": {
    "connector.class": "FileStreamSink",
    "tasks.max": "1",
    "topics": "input-topic",
    "file": "/tmp/test.sink.txt"
  }
}
```

* **name** â€” connector name
* **config** â€” map of properties, identical to Kafka Connect REST API

This ensures full compatibility with existing Kafka Connect workflows and tools.

---

## ğŸ”Œ Go Plugin Deployment

Connectors are built as **Go plugins** (`.so` files) that can be dynamically loaded at runtime.

* Build a connector as a Go plugin:

  ```bash
  go build -buildmode=plugin -o my-connector.so ./my-connector
  ```
* Place the `.so` file in the designated plugin directory.
* Reference it in your connector configuration.

This makes it easy to extend the system with custom connectors without modifying the core runtime.

---

## ğŸ³ Docker

Build and run with Docker:

```bash
docker build -t kafka-connector .
docker run --rm kafka-connector
```

---

## ğŸ§© Extensibility

The framework supports **plugins and registry** for extending functionality:

* Add new encoders/decoders under `encoding/`.
* Add transformations under `transform/`.
* Register and load connectors dynamically as Go plugins.

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ connector/       # Core source/sink connector logic
â”œâ”€â”€ encoding/        # Encoding & decoding implementations
â”œâ”€â”€ transform/       # Data transformations
â”œâ”€â”€ Dockerfile       # Container build
â”œâ”€â”€ go.mod           # Dependencies
â””â”€â”€ main.go          # Entry point
```

---

## ğŸ¤ Contributing

1. Fork the repo
2. Create a feature branch (`git checkout -b feature/my-feature`)
3. Commit changes (`git commit -m 'Add new feature'`)
4. Push to branch (`git push origin feature/my-feature`)
5. Open a Pull Request

---

## ğŸ“œ License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.

---

âœ… Now the README emphasizes **Kafka Connect REST API configs** directly, instead of tying things to a `config.yaml` file.

Do you also want me to create a **sample plugin implementation** (e.g., a simple FileSink connector in Go) so new users see exactly how to write and load a connector?
